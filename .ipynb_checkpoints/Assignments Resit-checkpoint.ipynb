{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resit Assignment (part 2)\n",
    "Foundations of Data Mining, Quartile 3, 2015-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decision trees (2 points)\n",
    "\n",
    "Answer the following question either with a formal proof or a (meaningful!) simulation. Assume you are dealing with a 2-class problem and you have to make a feature-independent decision about which class to predict. This comes up, for instance, when you are in a leaf node of a decision tree: you make a prediction by only looking at the class labels of your training data. Consider the two following rules:\n",
    "    - Always predict the largest class in the training data. \n",
    "    - Draw the class from the empirical probabilities of the classes in the training data. Hence, if one class occurs in 80% of the training data, you will predict that class in 80% of the predictions (and the other class in other predictions).\n",
    "\n",
    "Which rule gives the best performance? You can assume zero-one loss. Or do both rules always have the same performance?\n",
    "\n",
    "Hint: to prove this formally, calculate the espected error in both cases. If you want to answer with a simulation, make sure you consider different class distributions and sufficient repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: A small benchmark study (3 points)\n",
    "\n",
    "A benchmark study is an experiment in which multiple algorithms are evaluated on multiple datasets. The end goal is to study whether one algorithm is generally better than the others. Meaningful benchmark studies can grow quite complex, here we do a simplified variant.\n",
    "\n",
    "* Select at least five classification dataset. You can, for instance, download them from OpenML. They should be sufficiently large (e.g., at least 500 data points) so that the performance estimation is trustworthy.\n",
    "    * Hint: if you have no preference, a reasonable set seems to be datasets 37, 292, 470, 1480, and 1464\n",
    "* Select at least three classifiers that we discussed in class, e.g. kNN, Decision Trees, Random Forests, SVMs, Neural Networks. Note that some of these algorithms take longer to train. \n",
    "* Evaluate all classifiers (with default parameter settings) on all datasets, and count how many times each algorithm wins. Use an appropriate resampling procedure given the sizes of the datasets that you have chosen. Interpret and discuss your results.\n",
    "    * Hint: for the datasets mentioned above, a 10-fold cross-validation should be ok. On OpenML, the task IDs would be 37, 3019, 3561, 9971, and 10101.\n",
    "    * Hint: You can either compare the performances directly, or (better) use a statistical significance test, e.g. a pairwise t-test or (better) Wilcoxon signed ranks test, to see whether the performance differences are significant. This is covered in statistics courses. You can then count wins, ties and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: SVMs and optimization (3 points (1+1+1))\n",
    "\n",
    "Import the Wall Robot Navigation dataset from OpenML (http://www.openml.org/d/1497) into R or Python, and then create a separate independent test set with 20\\% of the instances using random stratified sampling. Then do the following:\n",
    "\n",
    "- Optimize the main hyperparameters of a Support Vector Machine (or SVMs) with a random search. Vary at least the main kernel types (linear, polynomial, and RBF) and the C parameter. Also vary the $\\gamma$ parameter for the RBF kernel and the exponent/degree for the polynomial kernel.\n",
    "    - Hint: values for C and $\\gamma$ are typically in [$2^{-15}..2^{15}$] on a log scale. The degree of the polynonial is typically in the range 2..10.\n",
    "    - Hint: just in case you cannot easily optimize the dependent kernel parameters together with the others, set up multiple random searches to optimize them separately, and take the maximum of those.\n",
    "- Use nested resampling on the training set to obtain a clean evaluation. Evaluate your optimized hyperparameter settings on the separate test set and discuss the result. Is the performance on the independent test set comparable with the result of the random search?\n",
    "    - Hint: for the nested resampling, use at least a 10-fold CV for the outer loop. The inner loop can be a 3-fold CV or a simple holdout.\n",
    "    - Hint: Each of the 10 folds may give you different optimized parameters, which you need to evaluate separately on the independent test set.\n",
    "- Discuss the effect of the hyperparameter tuning. Was it truly necessary to tune the hyperparameters? Which hyperparameters were most important to tune?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Letter recognition with Neural Networks (3 points (1+2))\n",
    "Train a neural network to make predictions for the letter dataset (http://www.openml.org/d/6).\n",
    "\n",
    "- Do a holdout, ideally with the first 16,000 examples as training data and the remaining 4,000 as test data. Report the classification error (percentage of wrong predictions).\n",
    "- Try setting the algorithm's hyperparameters (manually or automatically) that may help improve performance, such as momentum and alpha (large weight penalty). Explain your reasoning and show and interpret(!) the results.\n",
    "    - Hint: In Python, you can use scikit-learn's (development version, see Canvas) neural_network.MLPClassifier, or another library such as Keras. In R, you can use classif.nnTrain (this implementation allows stochastic gradient descent).\n",
    "    - Hint: Do a run with a few iterations first to see how long it takes to train and set the number of iterations to a large but feasible number. For instance, try a network with 1 layer of 50 hidden neurons for 100,000 stochastic gradient descent (SGD) steps. This can take a while to run.\n",
    "    - Hint: Most implementations allow you to set a batch size for stochastic gradient descent. Hence, with 20,000 data points you can use a batch size of 200 to do 100 steps per iteration (epoch), and 1,000 iterations to get 100,000 steps.\n",
    "    - Hint: Try a small constant learning rate (e.g. 0.01), or **if possible** a decaying learning rate (if your chosen implementation supports this)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
